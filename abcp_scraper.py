# abcp_scraper_threaded_commented.py

# –ò–º–ø–æ—Ä—Ç—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
import requests                               # –î–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤
from bs4 import BeautifulSoup                 # –î–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML
import pandas as pd                           # –î–ª—è –∑–∞–ø–∏—Å–∏ CSV
import json                                   # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON
import time                                   # –î–ª—è –ø–∞—É–∑—ã –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
import re                                     # –î–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
import logging                                # –î–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
from concurrent.futures import ThreadPoolExecutor, as_completed  # –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å
from requests.adapters import HTTPAdapter     # –î–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
from urllib3.util.retry import Retry          # –õ–æ–≥–∏–∫–∞ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
from config import BASE_URL, HEADERS, MAX_WORKERS, RETRY_ATTEMPTS, RETRY_DELAY, CSV_FILENAME, JSON_FILENAME

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ HTTP-—Å–µ—Å—Å–∏–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–≤—Ç–æ—Ä–æ–º –∑–∞–ø—Ä–æ—Å–æ–≤
session = requests.Session()
retries = Retry(total=RETRY_ATTEMPTS, backoff_factor=RETRY_DELAY, status_forcelist=[502, 503, 504])
adapter = HTTPAdapter(max_retries=retries)
session.mount("http://", adapter)
session.mount("https://", adapter)

# --- –§—É–Ω–∫—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –¥–æ —Ñ–æ—Ä–º–∞—Ç–∞ 7XXXXXXXXXX
def normalize_phone(phone_raw: str) -> str:
    phone = re.sub(r"[^\d]", "", phone_raw)  # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä
    if phone.startswith("8") and len(phone) == 11:
        phone = "7" + phone[1:]
    elif phone.startswith("9") and len(phone) == 10:
        phone = "7" + phone
    return phone if len(phone) == 11 else ""

# --- –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω-–ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤
def get_country_links():
    response = session.get(f"{BASE_URL}/suppliers", headers=HEADERS)
    soup = BeautifulSoup(response.text, "html.parser")
    country_links = []
    for group in soup.select("div.suppliers-btn-group"):
        for a in group.find_all("a", href=True):
            name = a.text.strip()
            href = a["href"]
            full_url = href if href.startswith("http") else BASE_URL + href
            country_links.append({"name": name, "url": full_url})
    return country_links

# --- –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞
def get_supplier_contact_info(profile_url):
    contact_info = {
        "–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–π —Å–∞–π—Ç": "",
        "–¢–µ–ª–µ—Ñ–æ–Ω—ã": [],
        "Email‚Äô—ã": []
    }

    # –ü—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ (–µ—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç)
    for attempt in range(1, RETRY_ATTEMPTS + 1):
        try:
            response = session.get(profile_url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            block = soup.find("div", class_="fr-panel-body")
            if not block:
                raise ValueError("–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")

            # –°–∞–π—Ç
            site_tag = block.select_one("a[href^='http']")
            if site_tag:
                contact_info["–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–π —Å–∞–π—Ç"] = site_tag.get_text(strip=True)

            phones = set()
            emails = set()

            # Email'—ã
            email_tags = block.select("a[href^='mailto']")
            for tag in email_tags:
                text = tag.text.strip()
                if "@" in text:
                    emails.add(text.lower())

            # –¢–µ–ª–µ—Ñ–æ–Ω—ã ‚Äî –∏–∑ —Å—Å—ã–ª–æ–∫ <a href="tel:...">
            tel_tags = block.select("a[href^='tel']")
            for tag in tel_tags:
                raw = tag.text.strip()
                phone_main = re.split(r"(–¥–æ–±\.?\s*\d+)", raw, maxsplit=1, flags=re.IGNORECASE)[0].strip()
                normalized = normalize_phone(phone_main)
                if normalized:
                    phones.add(normalized)

            # –¢–µ–ª–µ—Ñ–æ–Ω—ã ‚Äî –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–º
            for line in block.stripped_strings:
                if re.search(r"(—Ç–µ–ª–µ—Ñ–æ–Ω|mobile|phone|mob\.?):?\s*[\+\d\s\-\(\)]{6,}", line, flags=re.IGNORECASE):
                    matches = re.findall(r"(\+?\d[\d\-\s\(\)]{6,})", line)
                    for match in matches:
                        normalized = normalize_phone(match)
                        if normalized:
                            phones.add(normalized)

            contact_info["–¢–µ–ª–µ—Ñ–æ–Ω—ã"] = sorted(phones)
            contact_info["Email‚Äô—ã"] = sorted(emails)
            return contact_info

        except Exception as e:
            logging.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
            time.sleep(RETRY_DELAY)

    logging.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ—Ñ–∏–ª—å: {profile_url}")
    return contact_info

# --- –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ (–æ–¥–Ω–æ–≥–æ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞)
def parse_supplier(row, country_name):
    try:
        cols = row.find_all("td")
        if len(cols) < 8:
            return None

        number = cols[0].text.strip()
        rating_abcp = cols[1].text.strip()
        links = cols[2].select("a[href*='/suppliers']")
        name = links[1].text.strip() if len(links) > 1 else ""
        profile_url = links[1]["href"] if len(links) > 1 else ""
        if not profile_url.startswith("http"):
            profile_url = BASE_URL + profile_url
        site_span = cols[4].find("span")
        website = site_span.text.strip() if site_span else ""
        rating_span = row.select_one("span.starsBlock")
        rating = rating_span["title"].replace("–°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥", "").replace("&nbsp;", "").strip() if rating_span and rating_span.has_attr("title") else ""
        reviews_span = row.select_one("span.reviewsQuant")
        reviews = reviews_span.text.strip() if reviews_span else ""
        availability = cols[6].text.strip()
        response_time = cols[7].text.strip()

        logging.info(f"üë∑‚Äç‚ôÄÔ∏è –ü–æ—Ç–æ–∫: –ø–æ–ª—É—á–∞–µ–º {name}")
        contact_info = get_supplier_contact_info(profile_url)

        return {
            "ID": None,
            "–°—Ç—Ä–∞–Ω–∞": country_name,
            "–ù–æ–º–µ—Ä –≤ —Ç–∞–±–ª–∏—Ü–µ": number,
            "–†–µ–π—Ç–∏–Ω–≥ ABCP": rating_abcp,
            "–ù–∞–∑–≤–∞–Ω–∏–µ": name,
            "–°—Å—ã–ª–∫–∞": profile_url,
            "–°–∞–π—Ç": website,
            "–†–µ–π—Ç–∏–Ω–≥ (—á–∏—Å–ª–æ)": rating,
            "–û—Ç–∑—ã–≤—ã": reviews,
            "–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å": availability,
            "–í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ (—Å–µ–∫)": response_time,
            **contact_info
        }
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç—Ä–æ–∫–∏: {e}")
        return None

# --- –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤ –≤ —Å—Ç—Ä–∞–Ω–µ
def parse_suppliers_from_country(country_name, url):
    response = session.get(url, headers=HEADERS)
    soup = BeautifulSoup(response.text, 'html.parser')
    rows = soup.select("tr.distributor_row")

    suppliers = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_row = {executor.submit(parse_supplier, row, country_name): row for row in rows}
        for future in as_completed(future_to_row):
            supplier = future.result()
            if supplier:
                suppliers.append(supplier)

    return suppliers

# --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞
def main():
    all_suppliers = []
    countries = get_country_links()

    for country in countries:
        logging.info(f"üåç –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω—É: {country['name']}")
        try:
            suppliers = parse_suppliers_from_country(country["name"], country["url"])
            all_suppliers.extend(suppliers)
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω—ã {country['name']}: {e}")

    # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º ID
    for i, s in enumerate(all_suppliers, 1):
        s["ID"] = i

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã –∏ email –≤ –ø–ª–æ—Å–∫–∏–π –≤–∏–¥ (–¢–û–õ–¨–ö–û –¥–ª—è CSV)
    for supplier in all_suppliers:
        phones = supplier.get("–¢–µ–ª–µ—Ñ–æ–Ω—ã", [])
        emails = supplier.get("Email‚Äô—ã", [])
        for idx, phone in enumerate(phones, 1):
            supplier[f"–¢–µ–ª–µ—Ñ–æ–Ω {idx}"] = phone
        for idx, email in enumerate(emails, 1):
            supplier[f"Email {idx}"] = email
        supplier.pop("–¢–µ–ª–µ—Ñ–æ–Ω—ã", None)
        supplier.pop("Email‚Äô—ã", None)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV
    df = pd.DataFrame(all_suppliers)
    df.to_csv(CSV_FILENAME, sep=";", index=False, encoding="utf-8-sig")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON (–æ—Å—Ç–∞–≤–ª—è–µ–º —Å–ø–∏—Å–∫–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –∏ email'–æ–≤)
    for supplier in all_suppliers:
        supplier["–¢–µ–ª–µ—Ñ–æ–Ω—ã"] = supplier.pop("–¢–µ–ª–µ—Ñ–æ–Ω 1", None)
        supplier["Email‚Äô—ã"] = supplier.pop("Email 1", None)

    with open(JSON_FILENAME, "w", encoding="utf-8") as f:
        json.dump(all_suppliers, f, ensure_ascii=False, indent=4)

    logging.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {CSV_FILENAME} –∏ {JSON_FILENAME}")

# --- –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
if __name__ == "__main__":
    main()